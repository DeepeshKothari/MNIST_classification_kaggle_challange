{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing libraries\n",
    "import numpy as np\n",
    "import random\n",
    "import keras\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras import backend as K\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "#distributing data into test and train\n",
    "(x_train, y_train) , (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "#name of the output file\n",
    "output_file = \"submission.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(28000, 28, 28, 1)\n"
     ]
    }
   ],
   "source": [
    "#test data for evaluation\n",
    "\n",
    "test= pd.read_csv(\"test.csv\")\n",
    "mnist_testset = np.loadtxt('test.csv', skiprows=1, dtype='int', delimiter=',')\n",
    "test = mnist_testset.astype(\"float32\")\n",
    "test = test.reshape(-1, 28, 28, 1)/255.\n",
    "\n",
    "print(test.shape)\n",
    "# test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training data shape:\n",
      "(60000, 28, 28)\n",
      "(60000,)\n",
      "\n",
      "Test data shape:\n",
      "(10000, 28, 28)\n",
      "(10000,)\n"
     ]
    }
   ],
   "source": [
    "print (\"training data shape:\")\n",
    "print (x_train.shape)\n",
    "print (y_train.shape)\n",
    "print (\"\\nTest data shape:\")\n",
    "print (x_test.shape)\n",
    "print (y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5,1,'Y[1000] = 0')"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "img_num = 1000\n",
    "plt.imshow(X=x_train[img_num].reshape(28, 28), cmap=plt.get_cmap('gray'))\n",
    "plt.title(\"Y[\" + str(img_num) + \"] = \" + str(y_train[img_num]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28)\n",
      "(60000,)\n"
     ]
    }
   ],
   "source": [
    "#we have 60000 training and 10000 validation images of 28x28 size.\n",
    "\n",
    "idx = np.random.randint(x_train.shape[0], size=60000)\n",
    "x_train = x_train[idx, :]\n",
    "y_train = y_train[idx]\n",
    "\n",
    "print (x_train.shape)\n",
    "print (y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's specify input dimensions of each image\n",
    "img_rows, img_cols = 28,28\n",
    "input_shape = (img_rows, img_cols, 1)\n",
    "\n",
    "#also we can mention the batch size, number of classes and epochs\n",
    "batch_size = 128\n",
    "num_classes = 10\n",
    "epochs = 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28, 1)\n",
      "(10000, 28, 28, 1)\n"
     ]
    }
   ],
   "source": [
    "#reshaping the dataset for the model's input\n",
    "\n",
    "x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\n",
    "x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\n",
    "\n",
    "print (x_train.shape)\n",
    "print (x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's reshape y_train from (20000, ) to (20000, 10)\n",
    "\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "#converting pixels from int to float\n",
    "\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "\n",
    "#normalising\n",
    "x_train /= 255\n",
    "x_test /=225"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_13 (Conv2D)           (None, 26, 26, 32)        320       \n",
      "_________________________________________________________________\n",
      "conv2d_14 (Conv2D)           (None, 24, 24, 64)        18496     \n",
      "_________________________________________________________________\n",
      "conv2d_15 (Conv2D)           (None, 22, 22, 128)       73856     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2 (None, 11, 11, 128)       0         \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 11, 11, 128)       0         \n",
      "_________________________________________________________________\n",
      "flatten_5 (Flatten)          (None, 15488)             0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 128)               1982592   \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 10)                1290      \n",
      "=================================================================\n",
      "Total params: 2,076,554\n",
      "Trainable params: 2,076,554\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#bulding a model\n",
    "#model\n",
    "model = Sequential()\n",
    "\n",
    "# first layer needs to be told the input shape explicitly\n",
    "\n",
    "#first CNN layer\n",
    "\n",
    "model.add(Conv2D(32, kernel_size=(3,3),\n",
    "                activation = 'relu',\n",
    "                input_shape = input_shape))\n",
    "\n",
    "#second CNN layer\n",
    "\n",
    "model.add(Conv2D(64, kernel_size=(3,3),\n",
    "                activation = 'relu'))\n",
    "\n",
    "#third CNN layer\n",
    "model.add(Conv2D(128, kernel_size=(3,3),\n",
    "                activation = 'relu'))\n",
    "#adding a maxpool layer\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "#dropping out few neurons\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "#adding afully connected layer after flattining the input\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "#model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fitting the data on model\n",
    "model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer=keras.optimizers.Adadelta(),\n",
    "             metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/12\n",
      "47104/60000 [======================>.......] - ETA: 1:31 - loss: 0.2688 - acc: 0.9170"
     ]
    }
   ],
   "source": [
    "model.fit(x_train, y_train, batch_size=batch_size,\n",
    "         epochs = epochs,\n",
    "         verbose = 1,\n",
    "         validation_data = (x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#evaluating the model on the validation data\n",
    "\n",
    "model.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prediciting and storing the data into the submission CSV with header.\n",
    "y_hat = model.predict(test, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = np.argmax(y_hat,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(output_file, 'w') as f :\n",
    "    f.write('ImageId,Label\\n')\n",
    "    for i in range(len(y_pred)) :\n",
    "        f.write(\"\".join([str(i+1),',',str(y_pred[i]),'\\n']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
